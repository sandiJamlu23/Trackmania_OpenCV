{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code includes:\n",
    "\n",
    "1. Custom dataset class for COCO format\n",
    "2. Model initialization with pretrained weights\n",
    "3. Training and validation loops\n",
    "4. Learning rate scheduling\n",
    "5. Model checkpointing\n",
    "\n",
    "Key features:\n",
    "\n",
    "1. Uses Faster R-CNN with ResNet50 backbone\n",
    "2. Automatically maps your categories to model labels\n",
    "3. Saves the best model based on validation loss\n",
    "4. Shows progress bars during training\n",
    "5. Includes learning rate scheduling for better convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screenshots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screenshot capture started!\n",
      "Exiting program.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import keyboard\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a variable to control the screenshot loop\n",
    "capture = False\n",
    "\n",
    "while True:\n",
    "    # Check if the 's' key is pressed to start capturing\n",
    "    if keyboard.is_pressed('s'):\n",
    "        capture = True\n",
    "        print(\"Screenshot capture started!\")\n",
    "        time.sleep(0.2)  # Small delay to avoid multiple starts with one press\n",
    "\n",
    "    # Check if the 'q' key is pressed to stop capturing\n",
    "    if keyboard.is_pressed('q'):\n",
    "        capture = False\n",
    "        print(\"Screenshot capture stopped!\")\n",
    "        time.sleep(0.2)  # Small delay to avoid multiple stops with one press\n",
    "\n",
    "    # Take screenshots continuously if capture is True\n",
    "    if capture:\n",
    "        # Take a screenshot from the screen\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        screenshot = pyautogui.screenshot()\n",
    "        \n",
    "        # Convert screenshot to OpenCV format\n",
    "        screenshot_cv = cv2.cvtColor(np.array(screenshot), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Save the screenshot with a unique timestamp\n",
    "        screenshot_path = f\"D:/Thinkin in programming/Metopen/Traffic Signs/{timestamp}.png\"\n",
    "        cv2.imwrite(screenshot_path, screenshot_cv)\n",
    "        \n",
    "        # Add a delay to control the frequency of screenshots\n",
    "        time.sleep(1)  # Take a screenshot every 1 second (adjust as needed)\n",
    "    \n",
    "    # Exit the loop if the ESC key is pressed\n",
    "    if keyboard.is_pressed('esc'):\n",
    "        print(\"Exiting program.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/harshatejas/pytorch_custom_object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/trzy/FasterRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1YSpk-PiAyH9DAja_Okv-gM2mqbMsHWgb?authuser=1#scrollTo=Pm0QGw219EVU  Sandi Groot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/18ZEyu2oVV_iEpBTaSXL7Xvkf_-_jTfRC?usp=sharing sandijamlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import traceback\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL as Image\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Original category IDs: [0, 1, 2, 3]\n",
      "Category mapping: {0: 1, 1: 2, 2: 3, 3: 4}\n",
      "Found 834 valid images out of 834 total\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Original category IDs: [0, 1, 2, 3]\n",
      "Category mapping: {0: 1, 1: 2, 2: 3, 3: 4}\n",
      "Found 50 valid images out of 50 total\n",
      "Number of classes (including background): 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/834 [00:00<02:28,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor shape: torch.Size([3, 400, 400])\n",
      "Error in batch: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "CPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\n",
      "Meta: registered at /dev/null:184 [kernel]\n",
      "QuantizedCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:124 [kernel]\n",
      "BackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:153 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\n",
      "Functionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\n",
      "Named: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\n",
      "ZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:96 [backend fallback]\n",
      "AutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\n",
      "AutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\n",
      "AutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\n",
      "AutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\n",
      "AutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\n",
      "AutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\n",
      "AutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\n",
      "AutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\n",
      "AutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\n",
      "Tracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\n",
      "AutocastCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:34 [kernel]\n",
      "AutocastXPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:41 [kernel]\n",
      "AutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\n",
      "AutocastCUDA: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:27 [kernel]\n",
      "FuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\n",
      "BatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\n",
      "Batched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:207 [backend fallback]\n",
      "PythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:161 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\n",
      "PreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:165 [backend fallback]\n",
      "PythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:157 [backend fallback]\n",
      "\n",
      "Final tensor shape: torch.Size([3, 400, 400])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 2/834 [00:00<02:14,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "CPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\n",
      "Meta: registered at /dev/null:184 [kernel]\n",
      "QuantizedCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:124 [kernel]\n",
      "BackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:153 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\n",
      "Functionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:349 [backend fallback]\n",
      "Named: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\n",
      "ZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:96 [backend fallback]\n",
      "AutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\n",
      "AutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\n",
      "AutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\n",
      "AutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\n",
      "AutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\n",
      "AutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\n",
      "AutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:100 [backend fallback]\n",
      "AutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\n",
      "AutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\n",
      "Tracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\n",
      "AutocastCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:34 [kernel]\n",
      "AutocastXPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:41 [kernel]\n",
      "AutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\n",
      "AutocastCUDA: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:27 [kernel]\n",
      "FuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\n",
      "BatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\n",
      "Batched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:207 [backend fallback]\n",
      "PythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:161 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\n",
      "PreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:165 [backend fallback]\n",
      "PythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:157 [backend fallback]\n",
      "\n",
      "Final tensor shape: torch.Size([3, 400, 400])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 423\u001b[0m\n\u001b[0;32m    420\u001b[0m           \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 373\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    369\u001b[0m val_maps \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m    372\u001b[0m   \u001b[38;5;66;03m# save the model even validations fails\u001b[39;00m\n\u001b[1;32m--> 373\u001b[0m   train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    374\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    376\u001b[0m   \u001b[38;5;66;03m# Save checkpoint every N epochs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[36], line 263\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, scaler)\u001b[0m\n\u001b[0;32m    260\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    261\u001b[0m valid_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 263\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[36], line 103\u001b[0m, in \u001b[0;36mTrafficSignDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     96\u001b[0m     boxes \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(boxes, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     97\u001b[0m     labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m     99\u001b[0m     target \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m: boxes,\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: labels,\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([image_id]),\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[43mboxes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m boxes[:, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m*\u001b[39m (boxes[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m boxes[:, \u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miscrowd\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(boxes),), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m    105\u001b[0m     }\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, target\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        # Load COCO annotation\n",
    "        self.coco = COCO(annotation_file)\n",
    "        \n",
    "        # get images ids\n",
    "        self.image_ids = list(sorted(self.coco.imgs.keys()))\n",
    "        \n",
    "        # Get category mapping\n",
    "        self.category_ids = sorted(self.coco.getCatIds())\n",
    "        print(\"Original category IDs:\", self.category_ids)\n",
    "        \n",
    "        # Create a continuous mapping starting from 1 (0 is background)\n",
    "        self.category_id_to_label = {cat_id: idx + 1 for idx, cat_id in enumerate(self.category_ids)}\n",
    "        print(\"Category mapping:\", self.category_id_to_label)\n",
    "        \n",
    "        # Verify all images exist\n",
    "        self.valid_image_ids = []\n",
    "        for img_id in self.image_ids:\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "            if os.path.exists(img_path):\n",
    "                self.valid_image_ids.append(img_id)\n",
    "        \n",
    "        print(f\"Found {len(self.valid_image_ids)} valid images out of {len(self.image_ids)} total\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Load image\n",
    "            image_id = self.valid_image_ids[idx]\n",
    "            image_info = self.coco.loadImgs(image_id)[0]\n",
    "            image_path = os.path.join(self.root_dir, image_info['file_name'])\n",
    "            \n",
    "            # Read and process image\n",
    "            image = cv.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "            \n",
    "            image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "            image = cv.resize(image, (400, 400))\n",
    "            \n",
    "            # Convert to tensor\n",
    "            image = self.to_tensor(image)\n",
    "            \n",
    "            # Load annotations\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            \n",
    "            boxes = []\n",
    "            labels = []\n",
    "            \n",
    "            # Handle empty annotations\n",
    "            if not anns:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            else:\n",
    "                for ann in anns:\n",
    "                    x, y, w, h = ann['bbox']\n",
    "                    # Convert to x1, y1, x2, y2 format and normalize\n",
    "                    x1 = max(x, 0)\n",
    "                    y1 = max(y, 0)\n",
    "                    x2 = min(x + w, 400)\n",
    "                    y2 = min(y + h, 400)\n",
    "                    \n",
    "                    if x2 <= x1 or y2 <= y1:\n",
    "                        continue\n",
    "                    \n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                    label = self.category_id_to_label[ann['category_id']]\n",
    "                    labels.append(label)\n",
    "                \n",
    "                if boxes:  # Only convert if we have valid boxes\n",
    "                    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                    labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "                else:  # If all boxes were invalid\n",
    "                    boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                    labels = torch.zeros((0,), dtype=torch.int64)\n",
    "            \n",
    "            # Prepare target dictionary\n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': labels,\n",
    "                'image_id': torch.tensor([image_id]),\n",
    "                'area': torch.zeros((len(boxes),), dtype=torch.float32) if len(boxes) == 0 \n",
    "                       else (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),\n",
    "                'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "            }\n",
    "            \n",
    "            return image, target\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {str(e)}\")\n",
    "            # Return the next valid image instead\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "# Make a different class for readability man\n",
    "def get_model(num_classes):\n",
    "\n",
    "    # Load pre-trained model\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Relace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "def compute_ap(recalls, precisions):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves\"\"\"\n",
    "    ap = 0\n",
    "    for t in np.arange(0.0, 1.0, 0.01):\n",
    "        if np.sum(recalls >= t) == 0:\n",
    "            p = 0\n",
    "        else:\n",
    "            p = np.max(precisions[recalls >= t])\n",
    "\n",
    "        ap = ap + p / 11.\n",
    "\n",
    "\n",
    "def validate(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    # Dictionary to store all detections and ground truths\n",
    "    all_detections = defaultdict(list)  # class_id -> list of detections\n",
    "    all_ground_truths = defaultdict(list)  # class_id -> list of ground truths\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Validation\"):\n",
    "            images = list(image.to(device) for image in images)\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Process each image in the batch\n",
    "            for img_idx, (pred, target) in enumerate(zip(predictions, targets)):\n",
    "                # Get predictions for this image\n",
    "                boxes = pred['boxes'].cpu()\n",
    "                scores = pred['scores'].cpu()\n",
    "                labels = pred['labels'].cpu()\n",
    "\n",
    "                # Get ground truth for this image\n",
    "                gt_boxes = target['boxes'].cpu()\n",
    "                gt_labels = target['labels'].cpu()\n",
    "\n",
    "                # Store predictions and ground truths by class\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    all_detections[label.item()].append({\n",
    "                        'box': box,\n",
    "                        'score': score.item(),\n",
    "                        'img_idx': img_idx\n",
    "                    })\n",
    "\n",
    "                for box, label in zip(gt_boxes, gt_labels):\n",
    "                    all_ground_truths[label.item()].append({\n",
    "                        'box': box,\n",
    "                        'img_idx': img_idx,\n",
    "                        'matched': False  # Add matched flag instead of modifying tensor\n",
    "                    })\n",
    "\n",
    "    # Calculate AP for each class\n",
    "    aps = []\n",
    "\n",
    "    print(\"\\nCalculating AP for each class:\")\n",
    "    for class_id in all_ground_truths.keys():\n",
    "        detections = all_detections[class_id]\n",
    "        ground_truths = all_ground_truths[class_id]\n",
    "\n",
    "        # Skip if no ground truths for this class\n",
    "        if len(ground_truths) == 0:\n",
    "            continue\n",
    "\n",
    "        # Sort detections by score\n",
    "        detections = sorted(detections, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "        # Initialize arrays for precision-recall calculation\n",
    "        tp = np.zeros(len(detections))\n",
    "        fp = np.zeros(len(detections))\n",
    "\n",
    "        # Create dictionary of ground truth boxes per image\n",
    "        gt_per_img = defaultdict(list)\n",
    "        for gt_idx, gt in enumerate(ground_truths):\n",
    "            gt_per_img[gt['img_idx']].append({\n",
    "                'box': gt['box'],\n",
    "                'matched': False,\n",
    "                'idx': gt_idx\n",
    "            })\n",
    "\n",
    "        # Match detections to ground truths\n",
    "        for det_idx, detection in enumerate(detections):\n",
    "            img_gt_boxes = gt_per_img[detection['img_idx']]\n",
    "\n",
    "            if len(img_gt_boxes) == 0:\n",
    "                fp[det_idx] = 1\n",
    "                continue\n",
    "\n",
    "            # Get all ground truth boxes for this image\n",
    "            gt_boxes_tensor = torch.stack([gt['box'] for gt in img_gt_boxes])\n",
    "\n",
    "            # Calculate IoU with all ground truth boxes\n",
    "            iou = box_iou(detection['box'].unsqueeze(0), gt_boxes_tensor)\n",
    "\n",
    "            if len(iou) > 0:\n",
    "                max_iou = iou.max().item()\n",
    "                max_idx = iou.argmax().item()\n",
    "\n",
    "                if max_iou >= iou_threshold:\n",
    "                    # If this ground truth wasn't matched before\n",
    "                    if not img_gt_boxes[max_idx]['matched']:\n",
    "                        tp[det_idx] = 1\n",
    "                        # Mark this ground truth as matched\n",
    "                        img_gt_boxes[max_idx]['matched'] = True\n",
    "                        ground_truths[img_gt_boxes[max_idx]['idx']]['matched'] = True\n",
    "                    else:\n",
    "                        fp[det_idx] = 1\n",
    "                else:\n",
    "                    fp[det_idx] = 1\n",
    "            else:\n",
    "                fp[det_idx] = 1\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        tp_cumsum = np.cumsum(tp)\n",
    "        fp_cumsum = np.cumsum(fp)\n",
    "\n",
    "        recalls = tp_cumsum / len(ground_truths)\n",
    "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum)\n",
    "\n",
    "        # Add endpoints for AP calculation\n",
    "        precisions = np.concatenate([[1], precisions])\n",
    "        recalls = np.concatenate([[0], recalls])\n",
    "\n",
    "        # Calculate AP for this class using interpolation\n",
    "        ap = np.trapz(precisions, recalls)  # Using trapezoidal rule for AUC\n",
    "        aps.append(ap)\n",
    "\n",
    "        print(f\"Class {class_id}: AP = {ap:.4f}\")\n",
    "\n",
    "    # Calculate mAP\n",
    "    mAP = np.mean(aps) if aps else 0.0\n",
    "    print(f\"\\nMean Average Precision (mAP): {mAP:.4f}\")\n",
    "\n",
    "    return mAP\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    for images, targets in tqdm(data_loader, desc=\"Training\"):\n",
    "        try:\n",
    "            # Move images and targets to device\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Skip empty targets\n",
    "            if any(len(target['boxes']) == 0 for target in targets):\n",
    "                continue\n",
    "                \n",
    "            # Forward pass with autocast\n",
    "            with autocast(device_type=device.type):\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Skip if loss is NaN\n",
    "            if torch.isnan(losses):\n",
    "                continue\n",
    "                \n",
    "            # Backward pass with scaler\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(losses).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            total_loss += losses.item()\n",
    "            valid_batches += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in training batch: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return total_loss / valid_batches if valid_batches > 0 else float('inf')\n",
    "\n",
    "def main():\n",
    "    # Set device and enable blocking CUDA calls for better error messages\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Dataset paths\n",
    "    root_dir = \"D:/Thinkin in programming/R-CNN/train\"\n",
    "    train_annotation = \"D:/Thinkin in programming/R-CNN/train/_annotations.coco.json\"\n",
    "    val_root_dir = \"D:/Thinkin in programming/R-CNN/valid\"\n",
    "    val_annotation = \"D:/Thinkin in programming/R-CNN/valid/_annotations.coco.json\"\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = TrafficSignDataset(root_dir, train_annotation)\n",
    "    val_dataset = TrafficSignDataset(val_root_dir, val_annotation)\n",
    "\n",
    "    # Get number of classes\n",
    "    num_classes = len(train_dataset.category_ids) + 1\n",
    "    print(f\"Number of classes (including background): {num_classes}\")\n",
    "\n",
    "    # initialize scaler\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Create data loaders with smaller batch size\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda x: tuple(zip(*x)),\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = get_model(num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.0005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Initialize learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    best_map = 0.0\n",
    "\n",
    "    # Lists to store losses for plotting\n",
    "    train_losses = []\n",
    "    val_maps = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "      # save the model even validations fails\n",
    "      train_loss = train_one_epoch(model, optimizer, train_loader, device, scaler)\n",
    "      print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "      # Save checkpoint every N epochs\n",
    "      if (epoch + 1) % 2 == 0: # Save every 2 epochs\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "        }, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "\n",
    "      print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "      if torch.cuda.is_available():\n",
    "          torch.cuda.empty_cache()\n",
    "\n",
    "      try:\n",
    "          # Train\n",
    "          train_loss = train_one_epoch(model, optimizer, train_loader, device)\n",
    "          print(f\"Train Loss: {train_loss:.4f}\")\n",
    "          train_losses.append(train_loss)\n",
    "\n",
    "          # Validate with mAP\n",
    "          val_map = validate(model, val_loader, device)\n",
    "          print(f\"Validation mAP: {val_map:.4f}\")\n",
    "          val_maps.append(val_map)\n",
    "\n",
    "          # Update learning rate\n",
    "          lr_scheduler.step()\n",
    "\n",
    "          # Save best model (now based on mAP)\n",
    "          if val_map > best_map:\n",
    "              best_map = val_map\n",
    "              torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'best_map': best_map,\n",
    "                  'train_losses': train_losses,\n",
    "                  'val_maps': val_maps\n",
    "              }, 'best_model.pth')\n",
    "              print(\"Saved best model checkpoint\")\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"Error in epoch {epoch+1}: {e}\")\n",
    "          traceback.print_exc()\n",
    "          continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce RTX 2050\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_1956\\964141262.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 125\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction_result.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 101\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     98\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackground\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaution\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minformation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Path to test image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_path, num_classes)\u001b[0m\n\u001b[0;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39mroi_heads\u001b[38;5;241m.\u001b[39mbox_predictor \u001b[38;5;241m=\u001b[39m FastRCNNPredictor(in_features, num_classes)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load saved weights\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/best_model.pth'"
     ]
    }
   ],
   "source": [
    "def load_model(model_path, num_classes):\n",
    "  # initialize model\n",
    "  model = fasterrcnn_resnet50_fpn(pretrained = False)\n",
    "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "  # Load saved weights\n",
    "  checkpoint = torch.load(model_path)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def predict_image(model, image_path, device, confidence_threshold = 0.5):\n",
    "  # Load and preprocess the image\n",
    "  image = cv.imread(image_path)\n",
    "  image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "\n",
    "  # Convert to tensor and normalize\n",
    "  image_tensor = torch.from_numpy(image).float() / 255.0\n",
    "  image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "  # Move to device and set model to evaluation mode\n",
    "  model = model.to(device)\n",
    "  model.eval()\n",
    "  image_tensor = image_tensor.to(device)\n",
    "\n",
    "  # Get prediction\n",
    "  with torch.no_grad():\n",
    "      predictions = model(image_tensor)\n",
    "\n",
    "  # Extract boxes, scores, and labels from predictions\n",
    "  # Assuming predictions is a list of dictionaries, and the first element\n",
    "  # contains the predictions for the input image\n",
    "  boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "  scores = predictions[0]['scores'].cpu().numpy()\n",
    "  labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "  # Filter predictions based on confidence threshold\n",
    "  mask = scores >= confidence_threshold\n",
    "  boxes = boxes[mask]\n",
    "  scores = scores[mask]\n",
    "  labels = labels[mask]\n",
    "\n",
    "  return image, boxes, scores, labels\n",
    "\n",
    "def draw_predictions(image, boxes, scores, labels, class_names=None):\n",
    "    # Make a copy of the image to draw on\n",
    "    image_draw = image.copy()\n",
    "\n",
    "    # Define colors for different classes\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\n",
    "\n",
    "    # Total count for statistics\n",
    "    total_predictions = len(scores)\n",
    "    correct_predictions = sum([score >= 0.5 for score in scores])  # Confidence threshold\n",
    "\n",
    "    # Draw each prediction\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Convert box coordinates to integers\n",
    "        box = box.astype(np.int64)\n",
    "\n",
    "        # Get color for this class\n",
    "        color = colors[label % len(colors)]\n",
    "\n",
    "        # Draw box\n",
    "        cv.rectangle(image_draw, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "        # Prepare label text\n",
    "        if class_names and label < len(class_names):\n",
    "            label_text = f\"{class_names[label]}: {score:.2f}\"\n",
    "        else:\n",
    "            label_text = f\"Class {label}: {score:.2f}\"\n",
    "\n",
    "        # Draw label\n",
    "        cv.putText(image_draw, label_text, (box[0], box[1] - 10),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Calculate and display precision (percentage of correct predictions)\n",
    "    precision = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "    accuracy_text = f\"Accuracy: {precision:.2f}%\"\n",
    "\n",
    "    # Display accuracy on the image\n",
    "    cv.putText(image_draw, accuracy_text, (10, 30), cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    return image_draw\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Model path and number of classes (including background)\n",
    "    model_path = '/content/best_model.pth'\n",
    "    num_classes = 5  # Adjust this based on your model\n",
    "\n",
    "    # Class names (adjust these to match your classes)\n",
    "    class_names = ['background', 'other', 'caution', 'information', 'Warning']\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(model_path, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Path to test image\n",
    "    test_image_path = '/content/drive/MyDrive/R-CNN/Untitled.png'\n",
    "\n",
    "    # Get predictions\n",
    "    image, boxes, scores, labels = predict_image(model, test_image_path, device, confidence_threshold=0.5)\n",
    "\n",
    "    # Draw predictions with additional information\n",
    "    result_image = draw_predictions(image, boxes, scores, labels, class_names)\n",
    "\n",
    "    # Display results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(result_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save result\n",
    "    result_image_rgb = cv.cvtColor(result_image, cv.COLOR_RGB2BGR)\n",
    "    cv.imwrite('prediction_result.jpg', result_image_rgb)\n",
    "    print(\"Results saved to 'prediction_result.jpg'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REAL-TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation folder exists: True\n",
      "Validation annotation exists: True\n",
      "\n",
      "Validation annotation structure:\n",
      "Number of images: 50\n",
      "Number of annotations: 45\n",
      "Categories: [{'id': 0, 'name': 'warning-caution-information', 'supercategory': 'none'}, {'id': 1, 'name': 'caution', 'supercategory': 'warning-caution-information'}, {'id': 2, 'name': 'information', 'supercategory': 'warning-caution-information'}, {'id': 3, 'name': 'warning', 'supercategory': 'warning-caution-information'}]\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path, num_classes):\n",
    "    # Initialize model\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Load saved weights\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict_image(model, image, device, confidence_threshold=0.5):\n",
    "    # Convert to tensor and normalize\n",
    "    image_tensor = torch.from_numpy(image).float() / 255.0\n",
    "    image_tensor = image_tensor.permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "    # Move to device and set model to evaluation mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    # Extract boxes, scores, and labels from predictions\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "\n",
    "    # Filter predictions based on confidence threshold\n",
    "    mask = scores >= confidence_threshold\n",
    "    boxes = boxes[mask]\n",
    "    scores = scores[mask]\n",
    "    labels = labels[mask]\n",
    "\n",
    "    return boxes, scores, labels\n",
    "\n",
    "def draw_predictions(image, boxes, scores, labels, class_names=None):\n",
    "    # Make a copy of the image to draw on\n",
    "    image_draw = image.copy()\n",
    "\n",
    "    # Define colors for different classes\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\n",
    "\n",
    "    # Draw each prediction\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        # Convert box coordinates to integers\n",
    "        box = box.astype(np.int64)\n",
    "\n",
    "        # Get color for this class\n",
    "        color = colors[label % len(colors)]\n",
    "\n",
    "        # Draw box\n",
    "        cv.rectangle(image_draw, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "        # Prepare label text\n",
    "        if class_names and label < len(class_names):\n",
    "            label_text = f\"{class_names[label]}: {score:.2f}\"\n",
    "        else:\n",
    "            label_text = f\"Class {label}: {score:.2f}\"\n",
    "\n",
    "        # Draw label\n",
    "        cv.putText(image_draw, label_text, (box[0], box[1] - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    return image_draw\n",
    "\n",
    "\n",
    "def display_frame(frame):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "  plt.axis('off')  # Hide axis\n",
    "  plt.show()\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Model path and number of classes (including background)\n",
    "    model_path = '/content/best_model.pth'\n",
    "    num_classes = 5  # Adjust this based on your model\n",
    "\n",
    "    # Class names (adjust these to match your classes)\n",
    "    class_names = ['background', 'warning', 'caution', 'information', 'other']\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(model_path, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # Open webcam\n",
    "    cap = cv.VideoCapture(0)  # Change to a video file path for testing with videos\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get predictions\n",
    "        boxes, scores, labels = predict_image(model, frame_rgb, device, confidence_threshold=0.5)\n",
    "\n",
    "        # Draw predictions on the frame\n",
    "        result_frame = draw_predictions(frame, boxes, scores, labels, class_names)\n",
    "\n",
    "        # Display the frame with predictions\n",
    "        cv.imshow(\"Real-Time Object Detection\", result_frame)\n",
    "\n",
    "        # Break the loop when 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release webcam and close windows\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
